# in order to generate the documentation I used the following prompt

Write an architectural document describing a high throughput system for the purposes of a mock scenario. Do not include sample code. There will be 3 sections: the requirements, the design of the solution, and a description of a “prototype” of the larger solution.
In the first section about the requirements describe the following. The system will be capable of handling a million writes per minute with roughly 20kb of data in each request (Equivalent to roughly 20gb of data per minute) and will make the post-processed data available for query within 2 seconds. The processing includes deduplication of incoming records against existing data, attaching a unique patient ID, and storing the data in 2 places: one with all the data plus the unique ID, and another for public consumption which has the personal identifying information removed. In this highly unusual situation, a patient may be uniquely identified by their name and DOB alone, and the data contained about them is their favorite color. It is desirable that the state of the data can be snapshotted at a particular moment in time. In this highly unusual situation, it is guaranteed that there will be no race conditions where multiple duplicated patients get written to the database simultaneously.
In the second section about the design describe the following. Data will be ingested by Kafka because it provides fast ingestion and at this scale kafka would be a cheaper solution than kinesis. Also it would avoid proprietary vendor-lock-in and guarantee data is delivered exactly once. The trade-off is that we must manage it ourselves using AWS MSK (Managed Streaming for Kafka), or using the newly-introduced MSK Express. We only need to define one kafka topic. In production this Kafka will have Authentication, Authorization, and Encryption, and ACL’s. A spark cluster consumes data from the kafka topic and a spark cluster subscribes to that topic. The spark system will be secured with authentication, authorization (ACLs), encryption, and web UI security. It will be hosted in AWS EMR (Elastic Map Reduce). Using spark streaming we will read microbatches from the kafka topic, then check them in bulk against a redis cache to see whether or not they already exist in our system. If they do then we pair them with the unique patient ID, if they do not then we generate a new unique ID for it. Then we write the data to 3 sources simultaneously. One is a relational database that supports temporal tables and has high security security settings and strictly limited access that is logged for auditing. We add all of the data here including PHI. Since we want to take point-in time queries (or point-in-time recreations of the data) we would use either MariaDB or Postgres with the temporal extension. The second is a database that is stripped of PHI, that is queryable by analysts - here we put anonymized data (e.g. stripped of PHI and with the unique ID added). Since we want this data to be available for query as fast as possible it should go into a DynamoDB database. And the third is the Redis cache, where we add only the newly created unique ID’s. This system must pass a functional test where a million records of 20k each are written within a minute and the database is checked for those records within a 5 second window.
In the third section we discuss a subset of the above design that will be prototyped and how the components of the prototype correspond to the real-world system. The constraints of this prototype is that it is being done with an AWS budget of $0. There will be 3 docker containers: kafka, spark and postgres. The kafka container has one topic and zero security. It fills the place of the managed kafka service in the production design above. The spark container will have no security nor performance tuning. This container corresponds to the EMR cluster in the above technical specifications. The cluster with the postgres database will have two tables: one to represent the Redis cache and it will be a simple index of name+DOB storing the value of the unique ID. The other will represent the DynamoDB with only the unique ID and the anonymized data (which, in this case the data is only the favorite color field). An end-to-end test will be simulated by a python script that will send mocked data into Kafka, then wait 2 seconds, then query for that data in the database. This corresponds to a real-world functional test in the requirements above.
-----

